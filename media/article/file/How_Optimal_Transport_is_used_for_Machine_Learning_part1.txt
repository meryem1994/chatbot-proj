Abstract : Optimal transport (OT) theory focuses, among all maps T:Rd→Rd that can morph a probability measure onto another, on those that are the ``thriftiest’’, i.e. such that the averaged cost c(x,T(x)) between x and its image T(x) be as small as possible. Many computational approaches have been proposed to estimate such Monge maps when c is the ℓ22 distance, e.g., using entropic maps [Pooladian’22], or neural networks [Makkuva’20, Korotin’20]. We propose a new model for transport maps, built on a family of translation invariant costs c(x,y):=h(x−y), where h:=12∥⋅∥22+τ and τ is a regularizer. We propose a generalization of the entropic map suitable for h, and highlight a surprising link tying it with the Bregman centroids of the divergence Dh generated by h, and the proximal operator of τ. We show that choosing a sparsity-inducing norm for τ results in maps that apply Occam’s razor to transport, in the sense that the displacement vectors Δ(x):=T(x)−x they induce are sparse, with a sparsity pattern that varies depending on x. We showcase the ability of our method to estimate meaningful OT maps for high-dimensional single-cell transcription data, in the 34000-d space of gene counts for cells, without using dimensionality reduction, thus retaining the ability to interpret all displacements at the gene level

2.Optimal Transport Guided Unsupervised Learning for Enhancing low-quality Retinal Images (arXiv)

Author : Wenhui Zhu, Peijie Qiu, Mohammad Farazi, Keshav Nandakumar, Oana M. Dumitrascu, Yalin Wang

Abstract : Real-world non-mydriatic retinal fundus photography is prone to artifacts, imperfections and low-quality when certain ocular or systemic co-morbidities exist. Artifacts may result in inaccuracy or ambiguity in clinical diagnoses. In this paper, we proposed a simple but effective end-to-end framework for enhancing poor-quality retinal fundus images. Leveraging the optimal transport theory, we proposed an unpaired image-to-image translation scheme for transporting low-quality images to their high-quality counterparts. We theoretically proved that a Generative Adversarial Networks (GAN) model with a generator and discriminator is sufficient for this task. Furthermore, to mitigate the inconsistency of information between the low-quality images and their enhancements, an information consistency mechanism was proposed to maximally maintain structural consistency (optical discs, blood vessels, lesions) between the source and enhanced domains. Extensive experiments were conducted on the EyeQ dataset to demonstrate the superiority of our proposed method perceptually and quantitatively.